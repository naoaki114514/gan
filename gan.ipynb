{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda:0\nFiles already downloaded and verified\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "os.makedirs(\"./D0002G0002\", exist_ok=True)\n",
    "os.makedirs(\"./D0002G0002/generated_images\", exist_ok=True)\n",
    "os.makedirs(\"./D0002G0002/real_images\", exist_ok=True)\n",
    "save_path = \"./D0002G0002/loss.png\"\n",
    "\n",
    "# random seed 設定\n",
    "torch.manual_seed(1111)\n",
    "np.random.seed(1111)\n",
    "random.seed(1111)\n",
    "\n",
    "batch_size = 64 #一度に学習するデータ量\n",
    "loss_interval = 50 #test_lossを計算する間隔\n",
    "\n",
    "#変換器の作成\n",
    "transform = transforms.Compose([transforms.Resize(64),\n",
    "                                transforms.ToTensor(),  # torch.Tensor へ変換\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # 正規化する\n",
    " \n",
    "#訓練データのダウンロードと変換設定\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "#訓練データのローダ(読み込み器)の作成\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "nc = 3\n",
    "\n",
    "class Generator(nn.Module):\n",
    "  def __init__(self, nz):\n",
    "    super(Generator, self).__init__()\n",
    "    self.nz = nz\n",
    "    self.nf = 64\n",
    "    self.main = nn.Sequential(\n",
    "        nn.ConvTranspose2d(self.nz, self.nf * 8, 4, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(self.nf * 8),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(self.nf * 8, self.nf * 4, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(self.nf * 4),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(self.nf * 4, self.nf * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(self.nf * 2),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(self.nf * 2, self.nf, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(self.nf),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(self.nf, nc, 4, 2, 1, bias=False),\n",
    "        nn.Tanh() \n",
    "\n",
    "    )\n",
    "  def forward(self, input):\n",
    "    output = self.main(input)\n",
    "    return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.nf = 64\n",
    "    self.main = nn.Sequential(\n",
    "        nn.Conv2d(nc, self.nf, 4, 2, 1, bias = False),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        nn.Conv2d(self.nf, self.nf * 2, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(self.nf * 2),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        nn.Conv2d(self.nf * 2, self.nf * 4, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(self.nf * 4),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        nn.Conv2d(self.nf * 4, self.nf * 8, 4, 2, 1, bias = False),\n",
    "        nn.BatchNorm2d(self.nf * 8),\n",
    "        nn.LeakyReLU(0.2, inplace = True),\n",
    "        nn.Conv2d(self.nf * 8, 1, 4, 1, 0, bias = False),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "  def forward(self, input):\n",
    "    output = self.main(input)\n",
    "    return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "nz = 100\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device = device)#正規分布\n",
    "\n",
    "netG = Generator(nz).to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\"\"\"\n",
    "BCE(x, y) = -(y*log(x) + (1-y)*log(1-x))\n",
    "x cnn output\n",
    "y ground truth label\n",
    "\"\"\"\n",
    "#ネットワークを訓練する関数\n",
    "def train(netD, netG, criterion, optimizerD, optimizerG, n_epoch, batch):\n",
    "    netD.train()  # ネットワークを訓練状態へ切り替える\n",
    "    netG.train()  # ネットワークを訓練状態へ切り替える\n",
    "    D_loss = []\n",
    "    G_loss = []\n",
    "    for epoch in range(n_epoch):\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            if data[0].to(device).size()[0] != batch:\n",
    "              #一番最後はbatch_sizeに満たない場合は無視する\n",
    "              break\n",
    "\n",
    "            # Discriminatorの学習\n",
    "            # 本物を見分ける\n",
    "            optimizerD.zero_grad()\n",
    "            real = data[0].to(device)\n",
    "            batch_size = real.size()[0]\n",
    "           \n",
    "            label = torch.ones(batch_size, 1).to(device)\n",
    "            \n",
    "            output = netD(real)\n",
    "          \n",
    "            errD_real = criterion(output, label)#label = 1だと、BCEは-log(x)になるのでx = 1(本物を本物にしたい)に近くなると嬉しい\n",
    "\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # 偽物を見分ける\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)#正規分布\n",
    "          \n",
    "            fake = netG(noise)\n",
    "            label = torch.zeros(batch_size, 1).to(device)\n",
    "            output = netD(fake.detach())#勾配がGに伝わらないようにdetach()して止める\n",
    "            \n",
    "            errD_fake = criterion(output, label)#label = 0だと、BCEは-log(1-x)になるのでx = 0(偽物を偽物にしたい)に近くなると嬉しい\n",
    "            errD_fake.backward()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()#これでGのパラメータは更新されない\n",
    "\n",
    "            # Generatorの学習\n",
    "            optimizerG.zero_grad()\n",
    "            label = torch.ones(batch_size, 1).to(device)\n",
    "            output = netD(fake)\n",
    "\n",
    "            errG = criterion(output, label)#label = 1だと、BCEは-log(x)になるのでx = 1(偽物を本物にしたい、騙したい)に近くなると嬉しい\n",
    "            #実際の式とは少し異なる\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch+1, n_epoch, i, len(trainloader),\n",
    "                 errD.item(), errG.item()))\n",
    "  \n",
    "        fake = netG(fixed_noise)\n",
    "    \n",
    "        joined_real = torchvision.utils.make_grid(real, nrow=8, padding=3)\n",
    "        joined_fake = torchvision.utils.make_grid(fake, nrow=8, padding=3)\n",
    "        vutils.save_image(joined_fake.detach(), './D0002G0002/generated_images/fake_samples_epoch_%03d.png' % (epoch+1),normalize=True)\n",
    "        vutils.save_image(joined_real, './D0002G0002/real_images/real_samples_epoch_%03d.png' % (epoch+1), normalize=True)\n",
    "\n",
    "        D_loss.append(errD.item())\n",
    "        G_loss.append(errG.item())\n",
    "                   \n",
    "    print('Finished Training')\n",
    "    return D_loss, G_loss\n",
    "\n",
    "# 損失の変遷を表示する関数\n",
    "def show_loss(D_loss, G_loss, save_path):\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    x = [i*loss_interval for i in range(len(D_loss))]\n",
    "    plt.plot(x, D_loss, label='D_loss')\n",
    "    plt.plot(x, G_loss, label='G_loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D_loss, G_loss = train(netD, netG, nn.BCELoss(), optimizerD, optimizerG, n_epoch = 100, batch=batch_size)\n",
    "show_loss(D_loss, G_loss, save_path) # 損失の変遷を表示する\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}